3.1 节主要介绍了 **工具整合推理数据的合成方法**（Tool-Integrated Reasoning Data Synthesis Pipeline），目的是构建高质量的工具使用数据集，以支持多工具推理任务的训练。以下是该节的详细内容分析：

---

### **3.1 工具整合推理数据合成方法 (Tool-Integrated Reasoning Data Synthesis)**

这部分分为 **三个步骤**，分别是：

1. **数据收集与采样 (Step-1: Data Collection and Sampling)**  
2. **工具使用质量归一化 (Step-2: Tool-Use Quality Normalization)**  
3. **基于难度的数据分类 (Step-3: Difficulty-Aware Data Classification)**  

---

### **3.1.1 数据收集与采样**

#### 目标：
为了解决工具使用数据稀缺的问题，设计了一个 **数据采集与生成策略** 来自动生成工具使用的数据轨迹。

#### 数据来源：
从开源的知识推理和计算推理数据集中选取了大约 **90K条文本推理数据（Dtext）** 和 **1K条现成的工具整合推理数据（Dtool）**，如表 5 所示。

#### 采样方法：
为了进一步扩展包含工具调用的数据轨迹，采用了两种互补的采样策略：

1. **基于工具提示的采样 (TIR Prompting-based Sampling)：**
   - 使用工具整合提示符（**I_T**）来引导语言模型（LLM）生成带有工具调用的回答。
   - 过程：
     1. 在文本推理数据（Dtext）上使用提示符生成响应。
     2. 在生成过程中，模型会在特殊标记中调用工具，例如 `<search>` 和 `<python>`。
     3. 调用工具后，将工具返回的反馈结果插入推理链中（例如 `<result>`）。
     4. 迭代生成，直到达到最大工具调用次数或生成最终答案。
   - 最终，过滤掉错误的样本，得到数据集 **DP_tool**。

2. **基于提示的采样 (Hint-based Sampling)：**
   - 进一步多样化工具调用模式，使用提示（Hint）来引导工具调用。
   - 过程：
     1. 首先让模型对 Dtext 数据进行 **纯文本推理**。
     2. 在推理链中插入提示（Hint），例如：
        - **逻辑验证提示 (Logical Verification Hints)：** 替换推理链中的不确定表达（如“可能”、“不确定”），引导模型调用工具。
        - **答案反思提示 (Answer Reflection Hints)：** 在答案生成后插入提示，引导模型验证答案。
     3. 在提示位置截断原始推理链，模型需要从提示处继续推理，并调用工具完成任务。
   - 示例公式：
     - **Hint-based Tool-Integrated Reasoning**：
       ```
       Pθ(R_c>tH, y | I_T, q, R_c≤tH, T) = Y_Tc_t=tH Pθ(R_ct|R_c≤t, IT, q, {F_T}≤t)
       ```
     - 其中，**tH** 是提示插入的结束位置，R_c 是语言推理链，F_T 是工具反馈。
   - 最终，过滤掉错误样本，得到数据集 **DH_tool**。

#### 数据合并：
将两种采样方法生成的数据集合并，得到最终数据集 **Dv1_tool = {DH_tool ∪ DP_tool ∪ Dtool}**。

---

### **3.1.2 工具使用质量归一化 (Tool-Use Quality Normalization)**

#### 目标：
过滤和标准化工具使用数据，确保每个样本的工具调用逻辑合理。

#### 质量控制策略：
1. **工具调用频率控制 (Tool-call Frequency Control)：**
   - 移除工具调用频率超过预设阈值 β（例如，β = 5）的样本，避免过度调用工具。

2. **重复工具调用移除 (Duplicate Tool-call Removal)：**
   - 删除重复调用同一工具的样本，例如生成了相同的搜索查询或代码片段。

3. **格式归一化 (Format Normalization)：**
   - 规范推理链中工具调用的格式，例如统一 `<search>`、`<result>` 等标记的使用。
   - 确保工具调用的起始标记和结束标记匹配。

#### 输出：
经过这些步骤后，得到质量过滤后的数据集 **Dv2_tool**。

---

### **3.1.3 基于难度的数据分类 (Difficulty-Aware Data Classification)**

#### 目标：
根据工具调用的必要性和样本的难度，对数据进行分类，为后续的分阶段训练（从易到难）做好准备。

#### 分类标准：
将样本分为四类（见图 2），按难度递增：
1. **Category-1 (工具不必要，模型直接推理正确)：**  
   - 模型通过纯语言推理（Direct Reasoning, DR）即可得到正确答案，无需工具调用。
   - 从 Dv2_tool 中提取这些样本，构建子集 **Dsub_text**。

2. **Category-2 (工具不必要，但模型推理错误)：**
   - 模型的直接推理结果错误，工具调用也无法改进结果。

3. **Category-3 (工具必要且有效，模型直接推理错误)：**
   - 模型直接推理出错，但工具调用能够改进结果。
   - 从 Dv2_tool 中提取这些样本，构建子集 **Dsub_tool**。

4. **Category-4 (工具必要但仍困难)：**
   - 对于直接推理和工具调用均无法正确处理的样本，将其视为困难样本，保留用于强化学习阶段的训练，构建数据集 **DRL_tool**。

#### 数据集构建：
1. **冷启动微调数据集 (Cold-Start Fine-Tuning Dataset, DSFT_tool)：**
   - 合并 Category-1 和 Category-3 的样本：
     ```
     DSFT_tool = Dsub_text ∪ Dsub_tool
     ```
   - 用于指导模型初步学习工具使用。

2. **强化学习数据集 (Reinforcement Learning Dataset, DRL_tool)：**
   - 使用 Category-4 样本，帮助模型在强化学习阶段处理更复杂的任务。

---

### **总结**

通过上述三步，3.1 节设计了一套完整的工具整合推理数据合成流程：
- **第一步：采样**，生成多样化的工具调用轨迹；
- **第二步：归一化**，确保数据质量；
- **第三步：分类**，为分阶段训练准备数据。

这些步骤不仅解决了工具使用数据稀缺的问题，还为后续的两阶段训练框架提供了坚实的数据基础。

### **3.2 Tool-Star: 一个两阶段的多工具协作训练框架**

在 3.2 节中，**Tool-Star** 提出了一个两阶段的训练框架，旨在提升大语言模型（LLMs）在多工具协作任务中的推理能力。该框架分为两大阶段：

1. **冷启动监督微调阶段 (Cold-Start Supervised Fine-Tuning)**  
2. **多工具自评强化学习阶段 (Multi-Tool Self-Critic Reinforcement Learning)**  

以下是 3.2 节中各部分的详细解析。

---

### **背景与目标**

#### **问题定义：多工具协作推理 (Multi-tool Collaborative Reasoning)**

- **目标：**  
  模型在给定任务查询 $q$ 和工具集合 $T$ 的情况下，能够通过调用多个工具生成推理链 $R_c$，并最终生成答案 $y$。  
  推理过程可被建模为：
  $$
  P_\theta(R_c, y | I_T, q, T) = \prod_{t=1}^{T_c} P_\theta(R_c^t | R_c^{<t}, I_T, q, F_T^{<t}) \cdot \prod_{t=1}^{T_y} P_\theta(y_t | y_{<t}, R_c, I_T, q)$$
  $$
  $$
  - $R_c^{<t}$：推理链的前 $t-1$ 个 token。  
  - $F_T^{<t}$：工具调用反馈。
  $$

- **挑战：**
  1. **工具调用效率与成本平衡：** 模型需要在推理过程中高效调用工具，同时控制调用成本。
  2. **多工具协作能力：** 模型需要能够动态整合多个工具的功能，支持复杂推理任务。

#### **解决方案：两阶段训练框架**

Tool-Star 的两阶段训练框架旨在让模型从简单到复杂逐步学习工具协作推理：
1. **第一阶段：冷启动微调 (Cold-Start Fine-Tuning)**  
   指导模型初步学习工具调用和推理模式。
2. **第二阶段：强化学习 (Reinforcement Learning)**  
   通过强化学习增强模型的多工具协作推理能力。

---

### **3.2.1 冷启动监督微调 (Cold-Start Supervised Fine-Tuning)**

#### **目标：**
- 为模型提供初始的工具使用能力，帮助其探索基本的推理模式。

#### **方法：**
1. 给定微调数据集 $D_{SFT}^{\text{tool}}$，包括易于解决的工具调用样本。
2. 使用标准的监督微调目标函数：

   $$   
   \mathcal{L}(\theta) = -\sum_{(x_i, y_i) \in D_{SFT}^{\text{tool}}} \log P_\theta(y_i | x_i)
   $$
   - $x_i$：输入任务查询。
   - $y_i$：对应的正确输出，包括推理链和答案。

#### **结果：**
- 微调后的模型 $\hat{\pi}_\theta$ 能够初步掌握工具调用的推理能力（如调用搜索引擎、代码解释器等）。

---

### **3.2.2 多工具自评强化学习 (Multi-Tool Self-Critic Reinforcement Learning)**

#### **目标：**
- 让模型通过强化学习阶段学会自主调用和协作多个工具，提升其通用性和复杂任务解决能力。

#### **方法：**
强化学习阶段分为三个核心部分：
1. **基于记忆的工具调用 (Memory-based Roll-Out with Tools)**  
2. **分层奖励设计 (Hierarchical Reward Design)**  
3. **自评强化学习算法 (Self-Critic RL Algorithm)**  

---

#### **1. 基于记忆的工具调用**

- **过程：**
  - 在推理过程中，模型根据工具调用指令生成调用请求（如 `<search>` 或 `<python>`）。
  - 调用对应工具，并将工具反馈插入推理链中，供后续推理使用。
  - 为减少重复调用带来的延迟，引入**记忆机制**：
    - 缓存工具请求及其结果，重复调用时直接返回缓存内容。

---

#### **2. 分层奖励设计 (Hierarchical Reward Design)**

Tool-Star 的奖励设计分为三层：

1. **答案正确性奖励 (Answer Correctness Reward)：**
   - 如果模型生成的答案正确，给予奖励 $r_\text{correct}$。

2. **工具调用格式奖励 (Tool-Use Format Reward)：**
   - 如果模型的工具调用格式正确（如 `<search>` 和 `<result>` 标记匹配），给予奖励 $r_\text{format}$。

3. **多工具协作奖励 (Multi-Tool Collaboration Reward)：**
   - 如果模型在推理链中调用了多个工具（如同时使用了 `<search>` 和 `<python>`），给予额外奖励 $r_M = 0.1$。

**总体奖励公式：**
$$
R = 
\begin{cases} 
\max(r_\text{correct} + r_M, r_\text{correct}) & \text{如果格式正确且答案正确} \\
0 & \text{如果格式正确但答案错误} \\
-1 & \text{否则}
\end{cases}
$$    

---

#### **3. 自评强化学习算法 (Self-Critic RL Algorithm)**

为了帮助模型更好地适应复杂的奖励结构，Tool-Star 提出了**自评强化学习算法**，步骤如下：

1. **使用 GRPO 算法进行初步强化学习：**  
   - 采用分组相对策略优化（Group Relative Policy Optimization, GRPO）更新模型策略：
     $$
     \mathcal{L}_{\text{GRPO}}(\theta) = \mathbb{E}_{q \sim D, \{o_i\} \sim \pi_\theta} \left[ \frac{1}{G} \sum_{i=1}^G \frac{1}{|o_i|} \sum_{t=1}^{|o_i|} A_{i,t} \cdot \min\left(r_t, \text{clip}(r_t, 1 - \epsilon, 1 + \epsilon)\right) \right]
     $$
     - $A_{i,t}$：优势函数，衡量当前策略的改进方向。
     - $\epsilon$：用于限制策略更新的范围。

2. **引入自评奖励微调 (Self-Critic Reward Fine-Tuning)：**  
   - 在 GRPO 训练若干步后，模型进行自采样，生成多个候选推理链。
   - 使用分层奖励函数对每个推理链打分，构建奖励标注数据集 $D_\text{critic}^{\text{tool}}$。
   - 使用**对比偏好优化 (Direct Preference Optimization, DPO)** 目标进行微调：
     $$
     \mathcal{L}_{\text{DPO}} = -\mathbb{E}_{(x, y_w, y_l) \sim D} \left[ \log \sigma(\beta (\log \pi_\theta(y_w | x) - \log \pi_\theta(y_l | x))) \right]
     $$
     - $y_w$：偏好的输出（奖励高）。
     - $y_l$：不偏好的输出（奖励低）。

3. **交替训练：**  
   - 在标准 RL 和自评微调之间交替进行，逐步提升模型的推理能力。

---

### **3.2.3 总结与框架优势**

两阶段训练框架的核心优势：
1. **冷启动微调阶段：**  
   - 帮助模型快速掌握基本推理模式，降低强化学习的探索难度。

2. **强化学习阶段：**  
   - 通过分层奖励和自评优化，显著提升模型的多工具协作和复杂任务处理能力。

### **整体流程总结：**
1. **输入：** 任务查询 $q$、工具集合 $T$ 和初始模型 $\pi_\theta$。  
2. **第一阶段：冷启动微调，训练初始模型 $\hat{\pi}_\theta$。**  
3. **第二阶段：强化学习，交替进行 GRPO 和自评微调，最终获得多工具协作模型 $\pi_\theta^{RL}$。**

通过这个框架，Tool-Star 在多工具协作推理任务中实现了从易到难、从单工具到多工具的渐进式能力提升。



### 冷启动监督微调 SFT
在将我们的采样策略应用于原始数据集后，我们使用约 54k 个样本进行冷启动监督微调。在监督微调阶段，我们在训练数据上以 **7e-6 的学习率**微调一个 Qwen2.5-3B-Instruct 模型。我们使用 DeepSpeed ZeRO-3 [ 50] 和 FlashAttention2 [ 8] 进行训练。**批大小设置为 128，权重衰减设置为 0.1**，我们训练 **3 个 epoch**。采用 BF16 混合精度训练，并将**最大输入长度设置为 4096**。

在 SFT 之后，我们进一步在 VERL 框架 5 下使用 GRPO 算法训练模型。类似地，我们从 
$D_{tool}^{\text{RL}}$
 中随机选择约 10k 个样本用于强化学习。学习率设置为 8，每个样本进行 8 次 rollout。总训练批大小为 128，小批大小为 16。最大输出长度设置为 4096 以确保探索的完整性。为了加速训练速度，遵循 DAPO [ 77]，我们将训练期间工具调用的最大次数设置为 3。为了稳定训练，我们将 GRPO 中的 KL 散度系数设置为 0。强化学习阶段持续 2 个 epoch。所有实验均在 8 块 NVIDIA A800 GPU 上进行。为确保可重复性，所有实验结果均为三次独立试验的平均值。在 RL 阶段，验证集中约有 300 个样本，包括 AIME25、25、MATH500、HotpotQA、2wikiMultiHopQA、MuSiQue 和 Bamboogle。




It is worth noting that all tool invocation results in Tool-Star are masked. As illustrated in the Figure 3, these masked segments are excluded from the loss computation to prevent the training policy from being biased toward the tool outputs. Only the tokens from the text-based reasoning process and tool requests are considered during loss calculation.

**Tool-Star 在训练过程中不会将工具调用的结果（如 `<result>` 标签中的内容）纳入损失计算**。以下是具体的含义和原因：

---

### **1. 不计算工具调用结果的损失**
- **哪些部分被排除？**  
  工具调用的结果（即 `<result>` 标签中包含的内容）**不会参与损失计算**。具体来说：
  - 工具调用的结果（如搜索引擎返回的文本、代码执行的输出）会被插入到推理链中，但这些内容在计算训练损失时会被“屏蔽”（masked）。
  - 模型只基于它自己生成的推理链部分（如 `<think>` 中的内容）和工具调用请求（如 `<search>` 或 `<python>` 标签）来更新参数。

- **哪些部分参与计算？**  
  - 模型生成的**文本推理过程**（如 `<think>` 部分）。
  - 模型生成的**工具调用请求**（如 `<search>`、`<python>` 标签内的内容）。

---

### **2. 为什么要屏蔽工具调用结果？**

屏蔽工具调用结果的目的是为了避免模型在训练中对工具的返回结果产生**过度依赖**，具体体现在以下几个方面：

1. **防止训练策略偏向工具结果：**
   - 如果工具调用的结果也参与损失计算，模型可能会过度拟合工具返回的内容，导致它在推理过程中不充分利用自己的推理能力。
   - 通过屏蔽工具结果，模型需要独立学习如何生成合适的推理链和工具调用请求，而不是被工具结果“牵着走”。

2. **增强推理链和工具调用请求的质量：**
   - 由于损失只计算模型自身生成的推理链和工具请求，模型会优先优化这些部分的质量，确保生成的请求逻辑清晰且准确。
   - 例如，模型需要学习如何构造有效的搜索查询或生成正确的代码，而不是依赖工具结果的“反馈修正”。

3. **提高工具调用的鲁棒性：**
   - 工具调用结果在真实环境中可能会有不确定性（如搜索引擎返回的内容质量不同、代码执行失败等）。
   - 屏蔽工具结果后，模型会更注重优化自己的推理和工具调用能力，从而在实际使用中表现得更加稳定和可靠。

---

### **3. 具体示例**

假设模型在训练过程中生成如下推理链：

```plaintext
<think> I need to calculate the square root of 16. Let me use a Python interpreter. </think>
<python> import math; math.sqrt(16) </python>
<result> 4.0 </result>
<think> The result of the calculation is 4.0, so the square root of 16 is 4. </think>
<answer> The final answer is 4. </answer>
```

#### **计算损失时：**
- **参与损失计算的部分：**
  - `<think>` 部分的推理链。
  - `<python>` 部分的工具调用请求。
  - `<answer>` 部分的最终答案。

- **不参与损失计算的部分：**
  - `<result>` 部分中工具调用的返回结果（如 `4.0`）。

---

### **4. 这样设计的优势**

1. **强化模型独立推理能力：**  
   模型需要专注于生成高质量的推理链和工具调用请求，而不是依赖工具结果。

2. **增强泛化能力：**  
   通过屏蔽工具结果，模型在训练时不会对特定的工具反馈模式产生依赖，从而在面对不同工具实现或工具错误时更具适应性。

3. **避免策略作弊 (Strategy Hacking)：**  
   如果工具结果参与损失计算，模型可能会“投机取巧”，生成特定的工具调用请求以最大化损失函数，而不是专注于真实的任务目标。

---

### **总结**

这段话的核心是：
- **工具调用的结果是被屏蔽的，不参与损失计算。**
- **参与损失计算的只有模型生成的推理链和工具请求部分。**

这样设计的目的是为了让模型专注于优化自己的推理能力和工具调用请求质量，而不是依赖工具的输出结果。

---


### **DPO（Direct Preference Optimization）训练阶段** 

---

### **1. 学习率（Learning Rate）**
- 使用了 **5e-7** 的学习率：
  - 这个学习率较小，是因为 DPO 是在强化学习后的微调阶段，模型已经具有较强的推理能力，微调需要小步调整，以避免破坏已有的能力。
  - **学习率调度器：** 使用 **余弦调度器（cosine scheduler）**。
    - 余弦调度器可以在训练过程中逐渐降低学习率，帮助模型在训练后期更稳定地收敛。

---

### **2. 预热策略（Warm-Up Ratio）**
- **Warm-Up Ratio：0.1**
  - 意味着在前 10% 的训练步骤中，学习率从 0 增加到最大值（5e-7）。
  - 目的：避免训练初期的梯度振荡，确保模型优化的稳定性。

---

### **3. 使用的优化技术**
1. **DeepSpeed ZeRO Stage 3：**
   - DeepSpeed 是一种高效的深度学习优化工具，**ZeRO Stage 3** 是其内存优化技术的高级版本。
   - **作用：**
     - 将模型权重、优化器状态和激活函数分布到多块 GPU 上，降低显存占用。
     - 支持大规模模型的高效训练。

2. **Flash-Attention 2：**
   - FlashAttention 是一种高效的注意力机制实现，优化了 Transformer 的内存和计算效率。
   - **作用：**
     - 提升训练速度。
     - 减少显存占用，允许使用更大的上下文长度。

---

### **4. 批大小（Batch Size）**
- **全局批大小（Global Batch Size）：64**
  - 指所有 GPU 的总批量大小为 64。
  - 小批大小的选择是因为 DPO 微调的训练数据（正负样本对）较少，同时需要精细调整模型。

---

### **5. 损失函数（Loss Function）**
- **Sigmoid 损失函数：**
  - 用于实现样本对的二元分类（正偏好 vs. 负偏好），计算形式为：
    $$
    \mathcal{L}_{\text{DPO}} = -\log \sigma(\beta \cdot (\log \pi_\theta(y_w|x) - \log \pi_\theta(y_l|x)))
    $$
  - **Beta 值：0.3**
    - 控制正负样本对之间的偏好分数差异的影响。
    - 较小的 Beta 值表示在优化时对正负样本分数差异的敏感度较低，能够更平稳地训练模型。

---

### **6. 训练轮次与精度**
1. **训练轮次（Epochs）：2**
   - 训练时间较短，这是因为：
     - 自评阶段的目的是微调模型，进一步对奖励结构进行优化，而非从头训练。
     - 数据量相对较小，2 个 Epoch 足以使模型收敛。
   
2. **混合精度训练（Mixed Precision Training）：**
   - 使用 **bf16**（Brain Float 16）格式：
     - 减少显存占用，提高训练效率。
     - 同时保持数值稳定性，避免梯度溢出。

---

### **7. 最大上下文长度（Maximum Context Length）**
- **上下文长度：4096 tokens**
  - 表示模型最多可以处理 4096 个 token 的输入数据。
  - 这种较大的上下文长度允许模型处理更复杂的推理链和工具调用记录。

---

### **8. 自评训练频率（Self-Critic Training Frequency）**
- **每 0.5 个 Epoch 执行一次自评训练：**
  - 意味着在每 0.5 个训练轮次后，模型会进行一次自采样，并执行自评奖励微调。
  - 大约每 **50 步**（steps）执行一次：
    - 自评训练的频率较高，是为了帮助模型快速适应奖励结构并调整策略。
    - 自评训练后，使用更新的模型继续标准强化学习（Vanilla RL）。

---

### **9. 数据格式**
- **数据格式：**
  - 训练所用的数据格式在 **附录 E.2** 中进行了详细描述。
  - 数据包括输入任务 $x$、正偏好输出 $y_w$、负偏好输出 $y_l$：
    - $x$：任务查询。
    - $y_w$：正样本（奖励更高）。
    - $y_l$：负样本（奖励更低）。

---

### **检索器**

在训练过程中，我们使用 Bing Web Search API 作为检索器，并配置了 US-EN 区域设置。对于每个查询，我们检索 10 个网页作为支持文档。在推理阶段，我们使用 Bing Web Search 处理数学推理和复杂推理数据集。对于开放域问答，我们采用维基百科作为检索器。维基百科检索器基于 FlashRAG 提供的 2018 维基百科语料库[24]，并使用 E5-base-v2 作为检索模型。


### **推理阶段**
GAIA 数据集示例 #1

**问题：**  
寿命最长的脊椎动物是以一个岛屿的名字命名的。根据维基百科，该岛屿在2023年的人口数量是多少（精确到千位）？

---

**模型输出：**

为了回答这个问题，我需要识别出寿命最长的脊椎动物，并找出它所命名的岛屿，然后查找该岛屿在 2023 年的人口数据。

首先，我将确认哪种动物是已知最长寿的脊椎动物：

`<search>`

搜索查询：最长寿的脊椎动物是什么？

`</search>`

`<result>`

由哥本哈根大学领导、包括牛津大学在内的一组国际科学家发现，格陵兰鲨鱼的寿命至少为 272 年。这一发现表明它是科学界已知的最长寿脊椎动物，甚至超过了弓头鲸、海龟和陆龟。“我们的结果表明，格陵兰鲨鱼是寿命最长的脊椎动物之一，甚至超过了弓头鲸（Balaena mysticetus，估计寿命为 211 年），”尼尔森博士及其合著者表示。

`</result>`

由此可知，寿命最长的脊椎动物是**格陵兰鲨鱼**，它以**格陵兰岛**（Greenland）命名。

接下来，我将查找格陵兰岛在 2023 年的人口数据：

`<search>`

搜索查询：格陵兰岛 2023 年人口

`</search>`

`<result>`

截至 2023 年 1 月 1 日，格陵兰岛的人口为 56,609 人。一些来源还指出 2024 年的人口约为 55,840 人。由于题目要求的是 2023 年的数据，因此我们使用 56,609 作为参考值。

`</result>`

现在我将这个数字四舍五入到最近的千位：

`<python>`

```python
population = 56609
rounded_population = round(population, -3)
print(rounded_population)
```

`</python>`

`<result>`

57000

`</result>`

但注意，某些资料中提到 2023 年初的人口为 56,609，而其他来源则显示 2023 年底或 2024 年初的人口下降到了约 55,840。考虑到题目的时间点是“2023 年”，我们可以采用中间值 56,000 来代表这一年的近似人口数，并将其四舍五入到千位。

---

**最终答案：**

`<answer>`

56000

`</answer>`
 
