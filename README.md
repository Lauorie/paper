# Paper
Reading a paper is a delightful thing
---


1. [Style Transfer with Multi-iteration Preference Optimization](https://arxiv.org/pdf/2406.11581)

论文（简称 STAMP）提出了一种基于**多迭代偏好优化（SPO, Multi-iteration Preference Optimization）**的文本风格迁移训练框架。以下是 SPO 算法流程的详细说明，以及结合论文中提到的**hope-and-fear sampling（希望与恐惧采样）**的具体例子来阐释。

---

### **SPO 算法流程**
SPO 的核心在于通过多次迭代的偏好优化（Preference Optimization, PO），逐步优化模型，使其在目标风格、原文语义相似性以及流畅性等多目标上表现更优。以下是 SPO 的主要步骤：

---

#### **1. 初始化参考模型**
- 第一阶段先通过监督微调（Supervised Fine-tuning, SFT），利用伪平行数据（pseudo-parallel data）构建一个初始风格迁移模型 `fref`。
- 这个 `fref` 模型将作为 SPO 的初始参考模型，用于生成样本并构造偏好数据。

---

#### **2. 生成偏好数据**
在每一轮迭代中：
1. **生成候选样本**：
   - 使用参考模型 `fref` 对输入文本进行多次生成，得到若干输出候选文本。
   - 假设对某个输入文本 `xi`，生成了 5 个候选文本（`kPO=5`），分别是 `t1, t2, t3, t4, t5`。

2. **计算得分**：
   - 为每个候选文本计算以下三个目标的得分：
     - **目标风格强度（Target Style Strength, TSS）**：由风格分类器 `fcls` 评估。
     - **语义相似性（Meaning Similarity, MS）**：通过句向量的余弦相似度计算。
     - **流畅性（Fluency, F）**：通过一个流畅性分类器计算。
   - 通过加权乘积公式，将三个目标的得分聚合为总奖励 `R`：
     ```
     R = TSS^α · MS^β · F^γ
     ```
     其中，`α, β, γ` 是动态调整的权重参数，用于在多目标之间平衡。

3. **希望与恐惧采样（Hope-and-Fear Sampling）**：
   - 从候选文本中选择：
     - **希望样本（Hope Sample）**：总奖励 `R` 最大的样本。
     - **恐惧样本（Fear Sample）**：总奖励 `R` 最小的样本。
   - 比如，从 5 个候选文本中，假设得分如下：
     ```
     t1: R=0.8, t2: R=0.7, t3: R=0.9, t4: R=0.5, t5: R=0.6
     ```
     则希望样本为 `t3`（`R=0.9`），恐惧样本为 `t4`（`R=0.5`）。

4. **构造偏好对**：
   - 以希望样本 `tw`（如 `t3`）为正样本，恐惧样本 `tl`（如 `t4`）为负样本，构造偏好对 `(tw, tl)`。

---

#### **3. 偏好优化训练**
- 使用偏好优化算法（如 CPO）对模型进行训练：
  - 偏好优化的目标是让模型更倾向于生成希望样本，而非恐惧样本。
  - 优化目标为：
    ```
    L = -log(P(tw > tl))
    ```
    即最大化希望样本的概率。

---

#### **4. 更新参考模型**
- 用经过偏好优化后的模型替换参考模型 `fref`，然后进入下一轮迭代。
- 不断重复上述步骤，直到模型性能收敛或者达到预设的迭代次数。

---

### **结合例子说明 SPO 流程**
假设我们要将一句音乐歌词风格的句子：

**输入文本**：`I’m not sorry that it’s over.`  
目标风格：莎士比亚风格。

在 SPO 的某一轮迭代中：

1. **生成候选文本**：
   模型 `fref` 对输入文本生成以下候选：
   ```
   t1: "I feel no regret that it is gone."
   t2: "I lament not that it hath ended."
   t3: "I am not sorry that 'tis o'er."
   t4: "I regret it not, for it is done."
   t5: "I care not that it hath passed."
   ```

2. **计算得分**：
   - 每个候选文本的目标风格强度（TSS）、语义相似性（MS）和流畅性（F）得分如下：
     ```
     t1: TSS=0.6, MS=0.8, F=0.9
     t2: TSS=0.7, MS=0.7, F=0.8
     t3: TSS=0.85, MS=0.9, F=0.85
     t4: TSS=0.5, MS=0.7, F=0.75
     t5: TSS=0.65, MS=0.75, F=0.8
     ```
   - 计算总奖励 `R`（假设 α=1, β=1, γ=1）：
     ```
     t1: R = 0.6 * 0.8 * 0.9 = 0.432
     t2: R = 0.7 * 0.7 * 0.8 = 0.392
     t3: R = 0.85 * 0.9 * 0.85 = 0.650
     t4: R = 0.5 * 0.7 * 0.75 = 0.262
     t5: R = 0.65 * 0.75 * 0.8 = 0.390
     ```

3. **希望与恐惧采样**：
   - **希望样本（Hope Sample）**：`t3`（`R=0.650`，总得分最高）。
   - **恐惧样本（Fear Sample）**：`t4`（`R=0.262`，总得分最低）。

4. **构造偏好对**：
   - 偏好对为 `(t3, t4)`，即模型应更倾向生成 `t3` 而非 `t4`。

5. **偏好优化训练**：
   - 优化模型参数，使其在未来更倾向于生成类似 `t3` 的文本。

---

### **多迭代优化的效果**
经过多轮迭代，模型逐步生成更符合目标风格、语义相似性和流畅性的文本。例如，最终模型可能生成的结果为：

**最终输出**：`I am not sorry that 'tis o'er.`

相比初始生成的候选文本，最终输出更符合莎士比亚风格（使用了 "’tis" 和 "o’er"），同时保留了原文的核心语义。

---

### **总结**
SPO 的核心是通过多轮希望与恐惧采样构造偏好数据，结合偏好优化算法，逐步提升模型在多目标上的性能。通过动态调整权重和多轮优化，SPO 能有效平衡目标风格、语义相似性和流畅性，在文本风格迁移任务中取得了显著的效果。



2. [RaFe: Ranking Feedback Improves Query Rewriting for RAG](https://arxiv.org/pdf/2405.14431)

通过“排名反馈”（Ranking Feedback）优化查询重写（Query Rewriting）模型，以提升基于检索增强生成（Retrieval-Augmented Generation, RAG）任务的性能。以下是对论文主要内容的解读：

---

### **研究背景**
1. **RAG的重要性**：
   - RAG 将检索和生成结合起来，通过检索相关文档（外部知识）来增强生成任务的能力。RAG 被广泛应用于开放域问答（Open-Domain QA）等任务。
   - RAG 的核心流程是先检索相关文档，然后生成答案。然而，直接使用原始查询（original query）检索的文档可能不够准确或相关。

2. **查询重写**：
   - 查询重写的目标是将原始查询改写为更适合检索引擎的形式，以扩展检索到的文档集合。这种方法可以显著提升检索效果。
   - 当前查询重写的主流方法：
     - 使用大型语言模型 (LLMs) 直接生成查询重写（例如 few-shot 提示）。
     - 利用小型模型进行查询重写，并通过强化学习（Reinforcement Learning, RL）对其优化。

3. **现有方法的局限性**：
   - 现有强化学习方法通常依赖人工标注的文档、答案，或需要设计特定的奖励函数（reward），这些方法缺乏通用性。
   - 查询重写的反馈信号难以设计，导致模型难以有效优化。

---

### **论文贡献**
作者提出了一种名为 **RaFe** 的查询重写框架，核心思想是利用 **reranker（重排序器）** 提供的排名反馈信号来优化查询重写模型，具有以下特点：
- **无需人工标注数据**：不依赖标注的相关文档或答案，直接使用 reranker 提供的评分信号。
- **通用性强**：不需要特定领域的奖励设计，反馈信号可以泛化到不同任务和语言上。
- **性能提升显著**：通过实验，RaFe 在跨语言、多任务的开放域问答中表现优于现有基线。

---

### **方法：RaFe 框架**
RaFe 的核心是利用 reranker 提供的排名反馈进行查询重写模型的优化。具体分为两个阶段：

#### 1. **初始监督微调 (Supervised Fine-Tuning, SFT)**：
   - 使用生成的大型语言模型（如 Qwen）生成的查询重写数据，进行初始的监督微调（冷启动）。
   - 通过监督训练，学习最基本的查询重写能力。

#### 2. **反馈训练 (Feedback Training)**：
   - 利用 reranker 提供的反馈信号，优化查询重写模型。具体包括两种方式：
     1. **离线反馈训练 (Offline Feedback Training)**：
        - 使用 reranker 对文档进行评分，并区分 “好重写” 和 “坏重写”（通过一个阈值 µ 判断）。
        - 构造偏好对（preference pairs），通过强化学习方法（如 DPO 或 KTO）训练重写模型。
     2. **在线反馈训练 (Online Feedback Training)**：
        - 在生成重写后实时计算 reranker 的评分，通过 Proximal Policy Optimization (PPO) 方法优化重写模型。
   - Reranker 的反馈信号是基于“重写后检索到的文档排名分数的平均值”，与查询重写的目标（检索到更相关的文档）高度一致。

---

### **实验设置**
#### 1. **任务与数据集**：
- **任务**：主要验证在开放域问答任务中的表现。
- **数据集**：
  - **英文**：NQ、TriviaQA、HotpotQA、FreshQA 等。
  - **中文**：WebQA、FreshQA（翻译版）。
  - RaFe 跨领域、跨语言验证其通用性。

#### 2. **评价指标**：
- **问答性能**：使用 Exact Match (EM) 检测生成答案的准确性。
- **检索性能**：主要通过 Precision@K 和 MRR（Mean Reciprocal Rank）评价检索到的文档质量。

#### 3. **对比方法**：
- **OQR**：直接使用原始查询检索。
- **LLM-Rewrite**：使用大型语言模型（如 Qwen）生成查询重写。
- **Query2Doc**：通过 LLM 生成伪文档，再利用这些文档扩展查询。
- **SFT**：直接用生成的查询重写数据训练模型。

---

### **实验结果**
#### 1. **主要结果**
- RaFe 在几乎所有实验设置中都明显优于基线方法和原始查询检索（OQR）。
- 特别是 **EXPAND-Ranked（扩展+重排序）** 设置下，RaFe 的问答性能提升显著：
  - 在多个数据集上，RaFe 比次优方法高出 2%-3%。
  - 排名后的检索文档质量明显提高，证明 reranker 的反馈信号有效。

#### 2. **反馈信号的对比**
- 与其他反馈信号（如 QA 结果或检索精确度）相比，RaFe 的 reranker 反馈表现更好：
  - **LLM 反馈**：构造成本高（需要调用大型模型生成答案）。
  - **检索精确度反馈**：信号较弱，性能不如 reranker。
  - **reranker 反馈**：无需标注数据，成本低，效果最佳。

#### 3. **重写数量的影响**
- 实验表明，生成 2-3 个重写最能平衡性能与效率：
  - 更多的重写可以显著提升检索性能，但时间成本更高。

---

### **分析与优势**
1. **为何 reranker 反馈有效？**
   - Reranker 是传统检索系统的重要模块，能够根据查询和文档的相关性进行评分。
   - RaFe 利用 reranker 提供的分数作为自然反馈信号，将其转化为查询重写模型的优化目标。

2. **RaFe 的三种优势**：
   - **语义保持**：RaFe 的重写更能保持原始查询的语义。
   - **格式优化**：对查询格式进行调整，使其更适合检索引擎。
   - **多样性增强**：在不影响查询语义的前提下，生成多样化的重写。

---

### **相关工作**
1. **查询重写**：
   - 查询重写在传统信息检索中被广泛研究，近年来随着 LLM 兴起，RAG 场景下的查询重写备受关注。
   - 与现有方法相比，RaFe 不依赖人工标注或特定领域的奖励函数，具有更强的通用性。

2. **反馈学习**：
   - RaFe 借鉴了强化学习领域的反馈训练方法（如 RLHF），但创新地将 reranker 反馈引入到查询重写任务中。

---

### **结论与未来工作**
- **结论**：
  - RaFe 提出了一种无需标注的通用框架，通过 reranker 反馈优化查询重写模型，在开放域问答任务中达到了优异的性能。
  - 这一方法具有良好的跨语言和跨领域泛化能力。

- **未来工作**：
  - 探索 reranker 与查询重写模型的联合训练，以进一步提升检索和生成的整体性能。

---

### **总结**
这篇论文提出了一种创新的查询重写框架，通过 reranker 提供的排名反馈信号实现了高效、通用且无需标注的训练方法。在实际应用中，这一方法可以大幅提升检索增强生成（RAG）的性能，为开放域问答等任务提供更优的解决方案。
