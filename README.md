# paper
Reading a paper is a delightful thing
---


[Style Transfer with Multi-iteration Preference Optimization](https://arxiv.org/pdf/2406.11581)

论文（简称 STAMP）提出了一种基于**多迭代偏好优化（SPO, Multi-iteration Preference Optimization）**的文本风格迁移训练框架。以下是 SPO 算法流程的详细说明，以及结合论文中提到的**hope-and-fear sampling（希望与恐惧采样）**的具体例子来阐释。

---

### **SPO 算法流程**
SPO 的核心在于通过多次迭代的偏好优化（Preference Optimization, PO），逐步优化模型，使其在目标风格、原文语义相似性以及流畅性等多目标上表现更优。以下是 SPO 的主要步骤：

---

#### **1. 初始化参考模型**
- 第一阶段先通过监督微调（Supervised Fine-tuning, SFT），利用伪平行数据（pseudo-parallel data）构建一个初始风格迁移模型 `fref`。
- 这个 `fref` 模型将作为 SPO 的初始参考模型，用于生成样本并构造偏好数据。

---

#### **2. 生成偏好数据**
在每一轮迭代中：
1. **生成候选样本**：
   - 使用参考模型 `fref` 对输入文本进行多次生成，得到若干输出候选文本。
   - 假设对某个输入文本 `xi`，生成了 5 个候选文本（`kPO=5`），分别是 `t1, t2, t3, t4, t5`。

2. **计算得分**：
   - 为每个候选文本计算以下三个目标的得分：
     - **目标风格强度（Target Style Strength, TSS）**：由风格分类器 `fcls` 评估。
     - **语义相似性（Meaning Similarity, MS）**：通过句向量的余弦相似度计算。
     - **流畅性（Fluency, F）**：通过一个流畅性分类器计算。
   - 通过加权乘积公式，将三个目标的得分聚合为总奖励 `R`：
     ```
     R = TSS^α · MS^β · F^γ
     ```
     其中，`α, β, γ` 是动态调整的权重参数，用于在多目标之间平衡。

3. **希望与恐惧采样（Hope-and-Fear Sampling）**：
   - 从候选文本中选择：
     - **希望样本（Hope Sample）**：总奖励 `R` 最大的样本。
     - **恐惧样本（Fear Sample）**：总奖励 `R` 最小的样本。
   - 比如，从 5 个候选文本中，假设得分如下：
     ```
     t1: R=0.8, t2: R=0.7, t3: R=0.9, t4: R=0.5, t5: R=0.6
     ```
     则希望样本为 `t3`（`R=0.9`），恐惧样本为 `t4`（`R=0.5`）。

4. **构造偏好对**：
   - 以希望样本 `tw`（如 `t3`）为正样本，恐惧样本 `tl`（如 `t4`）为负样本，构造偏好对 `(tw, tl)`。

---

#### **3. 偏好优化训练**
- 使用偏好优化算法（如 CPO）对模型进行训练：
  - 偏好优化的目标是让模型更倾向于生成希望样本，而非恐惧样本。
  - 优化目标为：
    ```
    L = -log(P(tw > tl))
    ```
    即最大化希望样本的概率。

---

#### **4. 更新参考模型**
- 用经过偏好优化后的模型替换参考模型 `fref`，然后进入下一轮迭代。
- 不断重复上述步骤，直到模型性能收敛或者达到预设的迭代次数。

---

### **结合例子说明 SPO 流程**
假设我们要将一句音乐歌词风格的句子：

**输入文本**：`I’m not sorry that it’s over.`  
目标风格：莎士比亚风格。

在 SPO 的某一轮迭代中：

1. **生成候选文本**：
   模型 `fref` 对输入文本生成以下候选：
   ```
   t1: "I feel no regret that it is gone."
   t2: "I lament not that it hath ended."
   t3: "I am not sorry that 'tis o'er."
   t4: "I regret it not, for it is done."
   t5: "I care not that it hath passed."
   ```

2. **计算得分**：
   - 每个候选文本的目标风格强度（TSS）、语义相似性（MS）和流畅性（F）得分如下：
     ```
     t1: TSS=0.6, MS=0.8, F=0.9
     t2: TSS=0.7, MS=0.7, F=0.8
     t3: TSS=0.85, MS=0.9, F=0.85
     t4: TSS=0.5, MS=0.7, F=0.75
     t5: TSS=0.65, MS=0.75, F=0.8
     ```
   - 计算总奖励 `R`（假设 α=1, β=1, γ=1）：
     ```
     t1: R = 0.6 * 0.8 * 0.9 = 0.432
     t2: R = 0.7 * 0.7 * 0.8 = 0.392
     t3: R = 0.85 * 0.9 * 0.85 = 0.650
     t4: R = 0.5 * 0.7 * 0.75 = 0.262
     t5: R = 0.65 * 0.75 * 0.8 = 0.390
     ```

3. **希望与恐惧采样**：
   - **希望样本（Hope Sample）**：`t3`（`R=0.650`，总得分最高）。
   - **恐惧样本（Fear Sample）**：`t4`（`R=0.262`，总得分最低）。

4. **构造偏好对**：
   - 偏好对为 `(t3, t4)`，即模型应更倾向生成 `t3` 而非 `t4`。

5. **偏好优化训练**：
   - 优化模型参数，使其在未来更倾向于生成类似 `t3` 的文本。

---

### **多迭代优化的效果**
经过多轮迭代，模型逐步生成更符合目标风格、语义相似性和流畅性的文本。例如，最终模型可能生成的结果为：

**最终输出**：`I am not sorry that 'tis o'er.`

相比初始生成的候选文本，最终输出更符合莎士比亚风格（使用了 "’tis" 和 "o’er"），同时保留了原文的核心语义。

---

### **总结**
SPO 的核心是通过多轮希望与恐惧采样构造偏好数据，结合偏好优化算法，逐步提升模型在多目标上的性能。通过动态调整权重和多轮优化，SPO 能有效平衡目标风格、语义相似性和流畅性，在文本风格迁移任务中取得了显著的效果。
